#!/usr/bin/env python
import os, random
# os.environ["THEANO_FLAGS"] = "mode=FAST_RUN,device=gpu%d,floatX=float32"%(random.randint(0,3))
import tempfile, logging, sys

from kerlym import networks, agents, preproc, dqn, a3c

from optparse import OptionParser

parser = OptionParser()
parser.add_option("-e", "--env", dest="env", default="Pong-v0", help="Which GYM Environment to run [%default]")
parser.add_option("-n", "--net", dest="net", default="simple_dnn",
                  help="Which NN Architecture to use for Q-Function approximation [%default]")
parser.add_option("-b", "--batch_size", dest="bs", default=32, type='int',
                  help="Batch size durring NN training [%default]")
parser.add_option("-o", "--dropout", dest="dropout", default=0.5, type='float',
                  help="Dropout rate in Q-Fn NN [%default]")
parser.add_option("-p", "--epsilon", dest="epsilon", default=0.1, type='float',
                  help="Exploration(1.0) vs Exploitation(0.0) action probability [%default]")
parser.add_option("-D", "--epsilon_decay", dest="epsilon_decay", default=1e-6, type='float',
                  help="Rate of epsilon decay: epsilon*=(1-decay) [%default]")
parser.add_option("-s", "--epsilon_min", dest="epsilon_min", default=0.05, type='float',
                  help="Min epsilon value after decay [%default]")
parser.add_option("-d", "--discount", dest="discount", default=0.99, type='float',
                  help="Discount rate for future reards [%default]")
parser.add_option("-t", "--num_frames", dest="nframes", default=2, type='int',
                  help="Number of Sequential observations/timesteps to store in a single example [%default]")
parser.add_option("-m", "--max_mem", dest="maxmem", default=100000, type='int',
                  help="Max number of samples to remember [%default]")
parser.add_option("-P", "--plots", dest="plots", action="store_true", default=False,
                  help="Plot learning statistics while running [%default]")
parser.add_option("-F", "--plot_rate", dest="plot_rate", default=10, type='int',
                  help="Plot update rate in episodes [%default]")
parser.add_option("-a", "--agent", dest="agent", default="dqn", help="Which learning algorithm to use [%default]")
parser.add_option("-i", "--difference", dest="difference_obs", action="store_true", default=False,
                  help="Compute Difference Image for Training [%default]")
parser.add_option("-r", "--learning_rate", dest="learning_rate", type='float', default=1e-4,
                  help="Learning Rate [%default]")
parser.add_option("-E", "--preprocessor", dest="preprocessor", default="none", help="Preprocessor [%default]")
parser.add_option("-R", "--render", dest="render", action='store_true', default=False,
                  help="Render game progress [%default]")
parser.add_option("-c", "--concurrency", dest="nthreads", type='int', default=1,
                  help="Number of Worker Threads [%default]")
(options, args) = parser.parse_args()

print(options.agent)

training_dir = tempfile.mkdtemp()
logging.getLogger().setLevel(logging.DEBUG)

from gym import envs

agent_constructor = {
    "a3c": agents.A3C,
    "dqn": agents.DQN,
    "pg": agents.PG
}[options.agent]
preproc_ref = {
    "none": None,
    "atari": preproc.karpathy_preproc
}[options.preprocessor]

net = None
if options.agent == "dqn":
    net = eval("dqn.networks.%s" % (options.net))
elif options.agent == "a3c":
    net = eval("a3c.networks.%s" % (options.net))
elif options.agent == "pg":
    net = eval("networks.%s" % (options.net))
assert (not net == None)

print(net)
print(options.epsilon_decay)

agent = agent_constructor(
    experiment=options.env,
    env=lambda: envs.make(options.env),
    nthreads=options.nthreads,
    nframes=options.nframes,
    epsilon=options.epsilon,
    discount=options.discount,
    modelfactory=net,
    epsilon_schedule=lambda episode, epsilon: epsilon * (1 - options.epsilon_decay),
    batch_size=options.bs,
    dropout=options.dropout,
    stats_rate=options.plot_rate,
    enable_plots=options.plots,
    max_memory=options.maxmem,
    difference_obs=options.difference_obs,
    learning_rate=options.learning_rate,
    preprocessor=preproc_ref,
    render=options.render
)
agent.train()
